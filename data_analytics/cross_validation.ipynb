{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to perform cross validation for the monthly machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time so that various tasks can be timed\n",
    "import time\n",
    "\n",
    "# import math for mathematical calculates\n",
    "import math\n",
    "\n",
    "# import pandas and numpy for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import from sklearn for model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# import pickle so that models can be saved to file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. August Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/august_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [37.24188742999849, 37.30456476296007, 37.0961403936959]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. September Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/september_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [35.718245215854324, 36.328116655257524, 36.33918589277743]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. October Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/october_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [37.049191325513185, 36.66962245894936, 37.48971842633407]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. November Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/november_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [38.65456327223573, 38.27572719419434, 38.92495576433557]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. December Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/december_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [38.60321270351514, 38.158649739917216, 39.110517353038176]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. January Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/january_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [38.26150878231374, 36.26141625923627, 37.0530874155413]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. February Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/february_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [36.96773012986183, 37.01166482499384, 36.5499841435223]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. March Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/march_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [38.38497226913683, 39.165926791583516, 39.36675914835057]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. April Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/april_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'school_hol', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [37.55171376830806, 37.37205052756081, 37.406729724125675]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. May Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/may_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [37.36075688157742, 36.4515214256466, 37.64666091142253]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. June Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/june_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [36.94910566229633, 38.33545696621176, 37.979720827151276]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. July Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Import the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/data_analytics/data/july_bus_data_cleaned.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Encode the Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features we will use to train the model are:\n",
    "\n",
    "- hour\n",
    "- weekday\n",
    "- peak\n",
    "\n",
    "weekday & peak are already binary features. We will encode hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for hour\n",
    "df_dummies = pd.get_dummies(df['hour'], prefix='hour')\n",
    "df = pd.concat([df, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Prepare the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the descriptive & target features for the training data\n",
    "X = df[['actualtime_arr_stop_first','segment_mean','weekday', 'segment_std','peak', 'rain', 'temp', 'hour_0','hour_1','hour_4','hour_5','hour_6','hour_7','hour_8','hour_9','hour_10','hour_11','hour_12','hour_13','hour_14','hour_15','hour_16','hour_17','hour_18','hour_19','hour_20','hour_21','hour_22','hour_23']]\n",
    "y = df.time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation RMSE: [37.74627178730055, 38.42287886705406, 37.965746207415194]\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scaler = StandardScaler()\n",
    "gtb = GradientBoostingRegressor(max_depth=5, n_estimators=125)\n",
    "pipeline = Pipeline([('transformer', scaler), ('estimator', gtb)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "rmse = [math.sqrt(-1*x) for x in scores]\n",
    "print(\"\\nCross Validation RMSE:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
