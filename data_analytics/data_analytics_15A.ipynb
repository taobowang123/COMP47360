{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Bus Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os so that environment variables can be accessed (for database password, etc.)\n",
    "import os\n",
    "\n",
    "# import mysql connector so that data can be pulled into the notebook directly from the database\n",
    "import mysql.connector\n",
    "\n",
    "# import pandas and numpy for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib & seaborn for plotting and visualisaion\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# import transform_data function for transforming data into segment format\n",
    "from transform_data import transform_data\n",
    "\n",
    "# import convert_timestamp for various timestamp conversion functions\n",
    "import convert_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the max number of columns & rows to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 250)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for a bus route from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/route_15A.csv', sep=\";\", na_values=['\\\\N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a check to see how many rows and columns are in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print()\n",
    "print(\"Before any data cleaning, the CSV file contains\", rows, \"rows and\", cols, \"columns.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first five lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Checks on the Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Check for Duplicate Rows & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Duplicate rows:', df.duplicated()[df.duplicated() == True].shape[0])\n",
    "print('Duplicate columns:',df.columns.size - df.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns in the bus data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Check for Null/Empty Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with a count of zero can be dropped as they contain no useful information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['tenderlot', 'suppressed_trip', 'justificationid_trip', 'passengers', 'passengersin', 'passengersout', \\\n",
    "                      'distance_leavetimes', 'note_leavetimes', 'note_vehicle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continous features, and update the type of all categorical features to 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['datasource', 'dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'basin', \\\n",
    "                         'lastupdate_trip', 'note_trip', 'progrnumber', 'stoppointid', \\\n",
    "                          'suppressed_leavetimes', 'lastupdate_leavetimes']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Check for Constant Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with a count of one are constant and can be dropped. <br> **lineid** is constant for this subset of data, but is not constant for the full data set so will not be dropped at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['datasource', 'basin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Check for Constant Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no constant continuous features so nothing needs to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Further Analysis of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of features that can be dropped because they fall into at least one of the following categories:\n",
    "- Features that don't provide much information\n",
    "- Features that we won't be able to provide information on to the model\n",
    "\n",
    "These features can be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['lastupdate_trip', 'note_trip', 'suppressed_leavetimes', 'justificationid_leavetimes', \\\n",
    "                      'lastupdate_leavetimes','vehicleid', 'distance_vehicle', 'minutes_vehicle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initial Checks for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the categorical features and print details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'progrnumber', 'stoppointid']].columns\n",
    "\n",
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a full count for all categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the continuous features and print details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns\n",
    "\n",
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows missing data for **actualtime_arr_trip** and **actualtime_dep_trip**. This will be reviewed if these features are used in the future, currently they are not carried across when data is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform the Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus data must be transformed so that each row of data holds information on one journey segment. This is done by calling the *transform_data* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Check for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First re-assign the transformed data as continuous or categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_transformed[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', \\\n",
    "                          'progrnumber_next', 'stoppointid_next']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_transformed[column] = df_transformed[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_transformed.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_transformed[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df_transformed[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows with missing data, because the amount of missing rows is quite low, and because imputation would be difficult, these rows will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Drop Rows with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where *stoppointid_first* or *stoppointid_next* is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_first'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_next'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data is loaded from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open connection\n",
    "connection = mysql.connector.connect(host=os.environ['DBHOST'], user=os.environ['DBUSER'], \\\n",
    "        password=os.environ['DBPASS'], db='db_raw_data')\n",
    "\n",
    "# SQL query \n",
    "sql = \"SELECT * FROM weather_data \\\n",
    "WHERE record_date BETWEEN CAST('2018-01-01' AS DATE) AND CAST('2019-01-01' AS DATE);\"\n",
    "\n",
    "# load into dataframe\n",
    "df_weather = pd.read_sql(sql, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Check for Duplicate Rows & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Duplicate rows:', df_weather.duplicated()[df_weather.duplicated() == True].shape[0])\n",
    "print('Duplicate columns:',df_weather.columns.size - df_weather.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns so nothing needs to be dropped here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continuous features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'irain', 'itemp', 'iwb']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_weather.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Check for Missing Data, Constant Features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**itemp** and **iwb** are constant columns so can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate rows with missing data for rain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows where irain is not 0\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two rows where irain is not zero, these rows correspond to missing values for rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select other rows around the missing values\n",
    "df_weather[6220:6240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is no rain for the rest of the day, and given the high (for Ireland) temperature on the day, I think it's safe to replace the missing rain values with 0.\n",
    "\n",
    "I will then drop the feature **irain** as it provides no useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Replace Missing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace rain with 0 where irain is not 0\n",
    "df_weather['rain'].loc[df_weather['irain'] != 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that values are updated\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Drop Constant Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop(columns=['irain', 'itemp', 'iwb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Combine Bus and Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Split Timestamp for Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the data, timestamps must be split into month, day and hour.\n",
    "\n",
    "New features are added using lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['month'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_month_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['day'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_day_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['hour'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_hour_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features are updated to be categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'month', 'day', 'hour']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Split Timestamp for Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features are added using lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['month'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_month_bus(row['dayofservice'], \\\n",
    "                                                                                   row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['day'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_day_bus(row['dayofservice'], \\\n",
    "                                                                               row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['hour'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_hour_bus(\\\n",
    "                                                                                row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Merge the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_transformed, df_weather,  how='left', left_on=['month','day', 'hour'],\\\n",
    "                     right_on = ['month','day', 'hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are no rows missing weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.rain.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the data types for the new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_merged[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', 'progrnumber_next', 'stoppointid_next',\\\n",
    "                          'month', 'day', 'hour', 'record_date']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_merged[column] = df_merged[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_merged.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Day of Week Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['day_of_week'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_day_of_week(row['dayofservice']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Weekend/Weekday Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['weekday'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_weekday_weekend(row['dayofservice']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Bank Holiday Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make list of bank holidays for 2018 (based on https://www.officeholidays.com/countries/ireland/2018):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = ['2018-01-01 00:00:00', '2018-03-19 00:00:00', '2018-04-02 00:00:00', '2018-05-07 00:00:00', '2018-06-04 00:00:00',\\\n",
    "           '2018-08-06 00:00:00', '2018-10-29 00:00:00', '2018-12-25 00:00:00', '2018-12-26 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['bank_holiday'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_bank_holiday(row['dayofservice'], \\\n",
    "                                                                                                      holidays), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Time Difference Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['time_diff'] = df_merged.apply (lambda row: row['actualtime_arr_stop_next'] - row['actualtime_arr_stop_first'],\\\n",
    "                                          axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Update Data Types for the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_merged[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', 'progrnumber_next', 'stoppointid_next',\\\n",
    "                          'month', 'day', 'hour', 'record_date', 'day_of_week', 'weekday', 'bank_holiday']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_merged[column] = df_merged[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_merged.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Data Integrity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Time Difference Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for rows where the time difference is less than or equal to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[df_merged['time_diff'] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 167 rows that fail this test. These will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.loc[df_merged['time_diff'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Segment Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current 15A route visits the following stops:\n",
    "\n",
    "**Towards Ringsend:**\n",
    "\n",
    "1105,1107,1108,1109,1110,1111,1112,1113,1114,1115,2437,1117,1118,1119,1120,1164,1165,1166,1167,1168,\n",
    "1169,1170,1069,1070,1071,4528,1072,7577,1353,1354,7578,7582,0340,0350,0351,0352,0353,0354\n",
    "\n",
    "**Towards Limekiln Ave:**\n",
    "\n",
    "0395,0396,0397,0398,0399,0400,7581,1283,7579,1285,1016,1017,1018,1019,1020,1076,1077,1078,1079,1080,\n",
    "1081,1082,1083,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1101,1102,1103,1104\n",
    "\n",
    "We need to check that we have data for all stop combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have data for all stops going towards Ringsend\n",
    "stops = [1105,1107,1108,1109,1110,1111,1112,1113,1114,1115,2437,1117,1118,1119,1120,1164,1165,1166,1167,1168,1169,1170,1069,\\\n",
    "         1070,1071,4528,1072,7577,1353,1354,7578,7582,340,350,351,352,353,354]\n",
    "\n",
    "# loop through each combination of stops and print relevant data\n",
    "for i in range(len(stops)-1):\n",
    "    print(str(stops[i]) + \" to \" + str(stops[i+1]) + \": \")\n",
    "    df_temp = df_merged.loc[(df_merged['stoppointid_first'] == stops[i]) & (df_merged['stoppointid_next'] == stops[i+1])]\n",
    "    print(str(df_temp.shape[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have data for all stops going towards Limekiln Ave\n",
    "stops = [395,396,397,398,399,400,7581,1283,7579,1285,1016,1017,1018,1019,1020,1076,1077,1078,1079,1080, 1081,1082,1083,\n",
    "         1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1101,1102,1103,1104]\n",
    "\n",
    "# loop through each combination of stops and print relevant data\n",
    "for i in range(len(stops)-1):\n",
    "    print(str(stops[i]) + \" to \" + str(stops[i+1]) + \": \")\n",
    "    df_temp = df_merged.loc[(df_merged['stoppointid_first'] == stops[i]) & (df_merged['stoppointid_next'] == stops[i+1])]\n",
    "    print(str(df_temp.shape[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the route going towards **Limekiln Ave.** we have data for all segments.\n",
    "\n",
    "For the route going towards **Ringsend** we are missing data for the segment **1354 to 7578**.\n",
    "\n",
    "We also have less data than usual for the following segments:\n",
    "\n",
    "- **340 to 350**\n",
    "- **350 to 351**\n",
    "- **351 to 352**\n",
    "- **352 to 353**\n",
    "\n",
    "For these segments, we have approx. 3000 records compared with approx. 15000 for other segments.\n",
    "\n",
    "The missing segment seems to be due to a removed stop - 7589. It is possible that other routes may cover the segment that we are missing data for, however we may need to use timetable or real time information for this segment. There is also the possibility of using times between 1354 and 7589 and 7589 and 7578 to approximate travel time, but we would need more info on where stop 7589 was to see if this would be valid to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Plot Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Histograms for the Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[continuous_columns].hist(figsize=(30,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Box Plots for the Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous_columns:\n",
    "    f = df_merged[col].plot(kind='box', figsize=(10,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Bar Charts for the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features to plot\n",
    "# some categorical features will not be plotted here due to the high number of values e.g. bus stop no.\n",
    "categorical_columns_plot = df_merged[['lineid', 'routeid', 'direction',  \\\n",
    "                          'month', 'day', 'hour', 'day_of_week', 'weekday', 'bank_holiday']].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_columns_plot:\n",
    "    f = df_merged[column].value_counts().plot(kind='bar', title=column, figsize=(8,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Correlation Matrix for Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix using code found on https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Calculate correlation of all pairs of continuous features\n",
    "corr = df_merged[continuous_columns].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Generate a custom colormap - blue and red\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are mainly interested here in the correlation between the weather data and *time_diff* or *actualtime_arr_stop_next*. The correlation for these features with the weather data is quite low.\n",
    "\n",
    "Some of the weather features have very high correlation with each other e.g. temp with wetb and vappr with dewpt. The weather features we can use will be dependent on what is available in the weather forecast we receive. However, if all features are available in the forecast we may need to remove some of the features due to high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Correlations between the Continuous Features & the Target Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options for our target feature:\n",
    "\n",
    "- actualtime_arr_stop_next\n",
    "- time_diff\n",
    "\n",
    "I will plot scatter plots for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a scatter plot for each continuous feature against actualtime_arr_stop_next\n",
    "for col in continuous_columns:\n",
    "    df_merged.plot.scatter(x=col, y='actualtime_arr_stop_next', c='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a scatter plot for each continuous feature against time_diff\n",
    "for col in continuous_columns:\n",
    "    df_merged.plot.scatter(x=col, y='time_diff', c='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Correlations between the Categorical Features & the Target Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options for our target feature:\n",
    "\n",
    "- actualtime_arr_stop_next\n",
    "- time_diff\n",
    "\n",
    "I will plot box plots for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns_plot:\n",
    "    df_merged.boxplot(column=['actualtime_arr_stop_next'], by=[col], flierprops=flierprops, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns_plot:\n",
    "    df_merged.boxplot(column=['time_diff'], by=[col], flierprops=flierprops, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Deal with Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be investigated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Feature Selection for the Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial model, I will use the following features:\n",
    "\n",
    "- stoppointid_first\n",
    "- actualtime_arr_stop_first\n",
    "- stoppointid_next\n",
    "- actualtime_arr_stop_next (the target feature)\n",
    "- month\n",
    "- day_of_week             \n",
    "- weekday\n",
    "- bank_holiday\n",
    "- rain\n",
    "- temp\n",
    "- rhum\n",
    "- msl\n",
    "\n",
    "The following features have not been used for this initial model:\n",
    "\n",
    "- dayofservice: the relevant information from this feature has been split into other features e.g. month, day_of_week\n",
    "- tripid: we will not be able to provide this information to the model\n",
    "- lineid: constant as the initial model just covers one route\n",
    "- routeid: we may not be able to map this to the timetable information we have and if not won't be able to provide it to the model\n",
    "- direction: could look at including in a future model\n",
    "- progrnumber_first: probably shouldn't use as routes can change over time so progrnumber for a stop may not be the same as what the model was trained on\n",
    "- progrnumber_next: as above\n",
    "- day: could be included in a future model, but seems to have less correlation with other features compared to day_of_week\n",
    "- hour: could be included in a future model\n",
    "- record_date: the relevant information from this feature has been split into other features e.g. month, day_of_week\n",
    "- wetb: not available from OpenWeather\n",
    "- dewpt: not available from OpenWeather\n",
    "- vappr: not available from OpenWeather\n",
    "- time_diff: could look at including in a future model as target feature instead of actualtime_arr_stop_next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Prepare Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use binary encoding to avoid wrongly assigning an order to the categorical features where one isn't really present. The following features will be encoded:\n",
    "\n",
    "- stoppointid_first\n",
    "- stoppointid_next\n",
    "- month\n",
    "- day_of_week\n",
    "\n",
    "**weekday** and **bank_holiday** are already binary so don't need to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for stoppointid_first\n",
    "df_dummies = pd.get_dummies(df_merged['stoppointid_first'], prefix='stoppointid_first')\n",
    "df_merged = pd.concat([df_merged, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for stoppointid_next\n",
    "df_dummies = pd.get_dummies(df_merged['stoppointid_next'], prefix='stoppointid_next')\n",
    "df_merged = pd.concat([df_merged, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for month\n",
    "df_dummies = pd.get_dummies(df_merged['month'], prefix='month')\n",
    "df_merged = pd.concat([df_merged, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode values for day_of_week\n",
    "df_dummies = pd.get_dummies(df_merged['day_of_week'], prefix='day_of_week')\n",
    "df_merged = pd.concat([df_merged, df_dummies], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Data Quality Plan - Bus Data (Before Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| tenderlot       | All rows are null | Drop feature |\n",
    "| suppressed_trip | All rows are null | Drop feature |\n",
    "| justificationid_trip | All rows are null | Drop feature |\n",
    "| passengers | All rows are null | Drop feature | \n",
    "| passengersin | All rows are null | Drop feature |\n",
    "| passengersout | All rows are null | Drop feature |\n",
    "| distance_leavetimes | All rows are null | Drop feature |\n",
    "| note_leavetimes | All rows are null | Drop feature |\n",
    "| note_vehicle | All rows are null | Drop feature |\n",
    "| datasource | Constant feature | Drop feature |\n",
    "| lineid | Constant feature | This is constant because we just have data for one route loaded. At some point we may process more than one route together so will keep feature for now. May not be needed to train the model. |\n",
    "| basin | Constant feature | Drop feature |\n",
    "| lastupdate_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| note_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| suppressed_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| justifcationid_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| lastupdate_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| vehicleid | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| distance_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| minutes_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| actualtime_arr_trip | Missing values < 1% | Ignore for now as this feature is not brought across when data is transformed. |\n",
    "| actualtime_dep_trip | Missing values < 3% | Ignore for now as this feature is not brought across when data is transformed. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Data Quality Plan - Bus Data (After Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| stoppointid_first | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_first | Missing values ~ 1%| Drop affected rows |\n",
    "| stoppointid_next | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_next | Missing values ~ 1%| Drop affected rows |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 3: Data Quality Plan - Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| itemp | Constant feature | Drop feature |\n",
    "| iwb | Constant feature | Drop feature |\n",
    "| rain | Missing data - 2 rows | Imputation - replace with 0 after looking at data for other timestamps on the same day |\n",
    "| irain | Seems to be a missing data indicator | Drop feature as only two rows have missing data, and imputation is performed for these rows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 4: Data Quality Plan - After Merge & Features Added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| time_diff | For 167 rows, this feature is zero or negative | Drop affected rows|\n",
    "| All | Missing data for segment 1354 to 7578 due to stop 7589 no longer being on the route. | Decide on an approach - other routes may have data for this segment. If not, we may need to use timetable information or real time info to estimate travel time through this segment.  There is also the possibility of using times between 1354 and 7589 and 7589 and 7578 to approximate travel time, but we would need more info on where stop 7589 was to see if this would be valid to do. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 5: Tests for *transform_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df.loc[5:100]\n",
    "df_test1 = df_test1.reset_index(drop=True)\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed1 = transform.transform_data(df_test1)\n",
    "df_transformed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:35], df[42:100]]\n",
    "df_test2 = pd.concat(pieces)\n",
    "df_test2 = df_test2.reset_index(drop=True)\n",
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed2 = transform.transform_data(df_test2)\n",
    "df_transformed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[10:100]]\n",
    "df_test3 = pd.concat(pieces)\n",
    "df_test3 = df_test3.reset_index(drop=True)\n",
    "df_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed3 = transform.transform_data(df_test3)\n",
    "df_transformed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[8:10], df[14:50]]\n",
    "df_test4 = pd.concat(pieces)\n",
    "df_test4 = df_test4.reset_index(drop=True)\n",
    "df_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed4 = transform.transform_data(df_test4)\n",
    "df_transformed4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
