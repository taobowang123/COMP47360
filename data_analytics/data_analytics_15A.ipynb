{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & File Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import mysql.connector\n",
    "import transform\n",
    "\n",
    "\n",
    "# set max number of columns & rows to display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell to load data from a csv file\n",
    "df = pd.read_csv('./data/route_15A.csv', sep=\";\", na_values=['\\\\N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform a check to see how many rows and columns are in the file\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print()\n",
    "print(\"Before any data cleaning, the CSV file contains\", rows, \"rows and\", cols, \"columns.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Checks on the Data\n",
    "\n",
    "- Duplicate rows and columns\n",
    "- Null/empty features\n",
    "- Assign features as categorical or continuous\n",
    "- Constant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicate Rows & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print()\n",
    "print('Duplicate rows:', df.duplicated()[df.duplicated() == True].shape[0])\n",
    "# Check for duplicate columns\n",
    "print('Duplicate columns:',df.columns.size - df.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns so nothing needs to be dropped here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Null/Empty Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform a check for null/empty columns\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with count of zero can be dropped as they contain no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop null columns\n",
    "df = df.drop(columns=['tenderlot', 'suppressed_trip', 'justificationid_trip', 'passengers', 'passengersin', 'passengersout', 'distance_leavetimes', 'note_leavetimes', 'note_vehicle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print data types of all rows\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continous features, and update the type of all categorical features to 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['datasource', 'dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'basin', \\\n",
    "                         'lastupdate_trip', 'note_trip', 'progrnumber', 'stoppointid', \\\n",
    "                          'suppressed_leavetimes', 'lastupdate_leavetimes']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Constant Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop constant features\n",
    "df = df.drop(columns=['datasource', 'basin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Constant Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no constant continuous features so nothing needs to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features that don't provide much information\n",
    "- Features that we won't be able to provide information on to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features we won't use\n",
    "df = df.drop(columns=['lastupdate_trip', 'note_trip', 'suppressed_leavetimes', 'justificationid_leavetimes', \\\n",
    "                      'lastupdate_leavetimes','vehicleid', 'distance_vehicle', 'minutes_vehicle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Checks for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'progrnumber', 'stoppointid']].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a full count for all categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows missing data for **actualtime_arr_trip** and **actualtime_dep_trip**. This will be reviewed if these features are used in the future, currently they are not carried across when data is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform.transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First re-assign the transformed data as continuous or categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_transformed[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', \\\n",
    "                          'progrnumber_next', 'stoppointid_next']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_transformed[column] = df_transformed[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_transformed.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_transformed[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df_transformed[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows with missing data, because the amount of missing rows is quite low, and because imputation would be difficult, these rows will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Rows with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_first'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_next'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_transformed[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df_transformed[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load data directly from the database\n",
    "\n",
    "# open connection\n",
    "connection = mysql.connector.connect(host=os.environ['DBHOST'], user=os.environ['DBUSER'], \\\n",
    "        password=os.environ['DBPASS'], db='db_raw_data')\n",
    "\n",
    "# SQL query \n",
    "sql = \"SELECT * FROM weather_data \\\n",
    "WHERE record_date BETWEEN CAST('2018-01-01' AS DATE) AND CAST('2019-01-01' AS DATE);\"\n",
    "\n",
    "# load into dataframe\n",
    "df_weather = pd.read_sql(sql, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print()\n",
    "print('Duplicate rows:', df_weather.duplicated()[df_weather.duplicated() == True].shape[0])\n",
    "# Check for duplicate columns\n",
    "print('Duplicate columns:',df_weather.columns.size - df_weather.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns so nothing needs to be dropped here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'irain', 'itemp', 'iwb']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_weather.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Missing Data, Constant Columns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**itemp** and **iwb** are constant columns so can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate rows with missing data for rain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows where irain is not 0\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two rows where irain is not zero, these rows correspond to missing values for rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select other rows around the missing values\n",
    "df_weather[6220:6240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is no rain for the rest of the day, and given the high (for Ireland) temperature on the day, I think it's safe to replace the missing rain values with 0.\n",
    "\n",
    "I will then drop the feature **irain** as it provides no useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing data with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace rain with 0 where irain is not 0\n",
    "df_weather['rain'].loc[df_weather['irain'] != 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that values are updated\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features we won't use\n",
    "df_weather = df_weather.drop(columns=['irain', 'itemp', 'iwb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Bus and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split date out for Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to split out month, day and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_month(timestamp):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    return timestamp.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_day(timestamp):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    return timestamp.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_hour(timestamp):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    return timestamp.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns using lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['month'] = df_weather.apply (lambda row: timestamp_to_month(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['day'] = df_weather.apply (lambda row: timestamp_to_day(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['hour'] = df_weather.apply (lambda row: timestamp_to_hour(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update new columns to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'month', 'day', 'hour']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split date out for Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to split out month, day and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_hour_bus(seconds):\n",
    "    if seconds <= 86400:\n",
    "        hour = seconds // 3600\n",
    "    else:\n",
    "        hour = (seconds - 86400) // 3600\n",
    "    return hour % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_day_bus(timestamp, seconds):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    if seconds < 86400:\n",
    "        return timestamp.day\n",
    "    else:\n",
    "        if (timestamp.month in [1,3,5,7,8,10,12] and timestamp.day == 31) \\\n",
    "        or (timestamp.month in [4,6,9,11] and timestamp.day == 30) \\\n",
    "        or (timestamp.month == 2 and timestamp.day == 28):  # 2018 is not a leap year!\n",
    "            return 1\n",
    "        else:\n",
    "            return timestamp.day + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_month_bus(timestamp, seconds):\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "\n",
    "    if seconds >= 86400 and ((timestamp.month in [1,3,5,7,8,10,12] and timestamp.day == 31) \\\n",
    "    or (timestamp.month in [4,6,9,11] and timestamp.day == 30) \\\n",
    "    or (timestamp.month == 2 and timestamp.day == 28)):  # 2018 is not a leap year!\n",
    "        return timestamp.month + 1\n",
    "    else:\n",
    "        return timestamp.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns using lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['month'] = df_transformed.apply (lambda row: timestamp_to_month_bus(row['dayofservice'], \\\n",
    "                                                                                   row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['day'] = df_transformed.apply (lambda row: timestamp_to_day_bus(row['dayofservice'], \\\n",
    "                                                                               row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['hour'] = df_transformed.apply (lambda row: timestamp_to_hour_bus(row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_transformed, df_weather,  how='left', left_on=['month','day', 'hour'],\\\n",
    "                     right_on = ['month','day', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no rows missing weather data\n",
    "df_merged[df_merged.rain.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Plan - Bus Data (Before Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| tenderlot       | All rows are null | Drop feature |\n",
    "| suppressed_trip | All rows are null | Drop feature |\n",
    "| justificationid_trip | All rows are null | Drop feature |\n",
    "| passengers | All rows are null | Drop feature | \n",
    "| passengersin | All rows are null | Drop feature |\n",
    "| passengersout | All rows are null | Drop feature |\n",
    "| distance_leavetimes | All rows are null | Drop feature |\n",
    "| note_leavetimes | All rows are null | Drop feature |\n",
    "| note_vehicle | All rows are null | Drop feature |\n",
    "| datasource | Constant feature | Drop feature |\n",
    "| lineid | Constant feature | This is constant because we just have data for one route loaded. At some point we may process more than one route together so will keep feature for now. May not be needed to train the model. |\n",
    "| basin | Constant feature | Drop feature |\n",
    "| lastupdate_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| note_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| suppressed_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| justifcationid_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| lastupdate_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| vehicleid | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| distance_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| minutes_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| actualtime_arr_trip | Missing values < 1% | Ignore for now as this feature is not brought across when data is transformed. |\n",
    "| actualtime_dep_trip | Missing values < 3% | Ignore for now as this feature is not brought across when data is transformed. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Plan - Bus Data (After Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| stoppointid_first | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_first | Missing values ~ 1%| Drop affected rows |\n",
    "| stoppointid_next | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_next | Missing values ~ 1%| Drop affected rows |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Plan - Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| itemp | Constant feature | Drop feature |\n",
    "| iwb | Constant feature | Drop feature |\n",
    "| rain | Missing data - 2 rows | Imputation - replace with 0 after looking at data for other timestamps on the same day |\n",
    "| irain | Seems to be a missing data indicator | Drop feature as only two rows have missing data, and imputation is performed for these rows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for Transforming the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df.loc[5:100]\n",
    "df_test1 = df_test1.reset_index(drop=True)\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed1 = transform.transform_data(df_test1)\n",
    "df_transformed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:35], df[42:100]]\n",
    "df_test2 = pd.concat(pieces)\n",
    "df_test2 = df_test2.reset_index(drop=True)\n",
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed2 = transform.transform_data(df_test2)\n",
    "df_transformed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[10:100]]\n",
    "df_test3 = pd.concat(pieces)\n",
    "df_test3 = df_test3.reset_index(drop=True)\n",
    "df_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed3 = transform.transform_data(df_test3)\n",
    "df_transformed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[8:10], df[14:50]]\n",
    "df_test4 = pd.concat(pieces)\n",
    "df_test4 = df_test4.reset_index(drop=True)\n",
    "df_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed4 = transform.transform_data(df_test4)\n",
    "df_transformed4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
