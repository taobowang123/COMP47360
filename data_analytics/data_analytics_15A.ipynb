{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Bus Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os so that environment variables can be accessed (for database password, etc.)\n",
    "import os\n",
    "\n",
    "# import mysql connector so that data can be pulled into the notebook directly from the database\n",
    "import mysql.connector\n",
    "\n",
    "# import pandas and numpy for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import transform_data function for transforming data into segment format\n",
    "from transform_data import transform_data\n",
    "\n",
    "# import convert_timestamp for various timestamp conversion functions\n",
    "import convert_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the max number of columns & rows to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for a bus route from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/route_15A.csv', sep=\";\", na_values=['\\\\N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a check to see how many rows and columns are in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print()\n",
    "print(\"Before any data cleaning, the CSV file contains\", rows, \"rows and\", cols, \"columns.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first five lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Checks on the Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Check for Duplicate Rows & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Duplicate rows:', df.duplicated()[df.duplicated() == True].shape[0])\n",
    "print('Duplicate columns:',df.columns.size - df.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns in the bus data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Check for Null/Empty Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with a count of zero can be dropped as they contain no useful information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['tenderlot', 'suppressed_trip', 'justificationid_trip', 'passengers', 'passengersin', 'passengersout', \\\n",
    "                      'distance_leavetimes', 'note_leavetimes', 'note_vehicle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continous features, and update the type of all categorical features to 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['datasource', 'dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'basin', \\\n",
    "                         'lastupdate_trip', 'note_trip', 'progrnumber', 'stoppointid', \\\n",
    "                          'suppressed_leavetimes', 'lastupdate_leavetimes']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Check for Constant Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with a count of one are constant and can be dropped. <br> **lineid** is constant for this subset of data, but is not constant for the full data set so will not be dropped at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['datasource', 'basin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Check for Constant Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no constant continuous features so nothing needs to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Further Analysis of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of features that can be dropped because they fall into at least one of the following categories:\n",
    "- Features that don't provide much information\n",
    "- Features that we won't be able to provide information on to the model\n",
    "\n",
    "These features can be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['lastupdate_trip', 'note_trip', 'suppressed_leavetimes', 'justificationid_leavetimes', \\\n",
    "                      'lastupdate_leavetimes','vehicleid', 'distance_vehicle', 'minutes_vehicle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initial Checks for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the categorical features and print details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'progrnumber', 'stoppointid']].columns\n",
    "\n",
    "# Print details for the categorical columns\n",
    "df[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a full count for all categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the continuous features and print details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df.select_dtypes(['float64', 'int64']).columns\n",
    "\n",
    "# Print details for the continuous columns\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows missing data for **actualtime_arr_trip** and **actualtime_dep_trip**. This will be reviewed if these features are used in the future, currently they are not carried across when data is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform the Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus data must be transformed so that each row of data holds information on one journey segment. This is done by calling the *transform_data* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Check for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First re-assign the transformed data as continuous or categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_transformed[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', \\\n",
    "                          'progrnumber_next', 'stoppointid_next']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_transformed[column] = df_transformed[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_transformed.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_transformed[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the continuous columns\n",
    "df_transformed[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows with missing data, because the amount of missing rows is quite low, and because imputation would be difficult, these rows will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Drop Rows with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where *stoppointid_first* or *stoppointid_next* is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_first'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed[pd.notnull(df_transformed['stoppointid_next'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data is loaded from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open connection\n",
    "connection = mysql.connector.connect(host=os.environ['DBHOST'], user=os.environ['DBUSER'], \\\n",
    "        password=os.environ['DBPASS'], db='db_raw_data')\n",
    "\n",
    "# SQL query \n",
    "sql = \"SELECT * FROM weather_data \\\n",
    "WHERE record_date BETWEEN CAST('2018-01-01' AS DATE) AND CAST('2019-01-01' AS DATE);\"\n",
    "\n",
    "# load into dataframe\n",
    "df_weather = pd.read_sql(sql, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Check for Duplicate Rows & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Duplicate rows:', df_weather.duplicated()[df_weather.duplicated() == True].shape[0])\n",
    "print('Duplicate columns:',df_weather.columns.size - df_weather.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows or columns so nothing needs to be dropped here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Assign Features as Continuous or Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data types of all rows after the file import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign categorical and continuous features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'irain', 'itemp', 'iwb']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_weather.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Check for Missing Data, Constant Features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[categorical_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**itemp** and **iwb** are constant columns so can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details for the categorical columns\n",
    "df_weather[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate rows with missing data for rain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows where irain is not 0\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two rows where irain is not zero, these rows correspond to missing values for rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select other rows around the missing values\n",
    "df_weather[6220:6240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is no rain for the rest of the day, and given the high (for Ireland) temperature on the day, I think it's safe to replace the missing rain values with 0.\n",
    "\n",
    "I will then drop the feature **irain** as it provides no useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Replace Missing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace rain with 0 where irain is not 0\n",
    "df_weather['rain'].loc[df_weather['irain'] != 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that values are updated\n",
    "df_weather.loc[df_weather['irain'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Drop Constant Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop(columns=['irain', 'itemp', 'iwb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Combine Bus and Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Split Timestamp for Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the data, timestamps must be split into month, day and hour.\n",
    "\n",
    "New features are added using lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['month'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_month_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['day'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_day_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['hour'] = df_weather.apply (lambda row: convert_timestamp.timestamp_to_hour_weather(row['record_date']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features are updated to be categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_weather[['record_date', 'month', 'day', 'hour']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_weather[column] = df_weather[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Split Timestamp for Bus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features are added using lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['month'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_month_bus(row['dayofservice'], \\\n",
    "                                                                                   row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['day'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_day_bus(row['dayofservice'], \\\n",
    "                                                                               row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['hour'] = df_transformed.apply (lambda row: convert_timestamp.timestamp_to_hour_bus(\\\n",
    "                                                                                row['actualtime_arr_stop_first']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Merge the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_transformed, df_weather,  how='left', left_on=['month','day', 'hour'],\\\n",
    "                     right_on = ['month','day', 'hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are no rows missing weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.rain.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the data types for the new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df_merged[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction',  \\\n",
    "                         'progrnumber_first', 'stoppointid_first', 'progrnumber_next', 'stoppointid_next',\\\n",
    "                          'month', 'day', 'hour', 'record_date']].columns\n",
    "\n",
    "# Convert data type to 'Category' for these columns\n",
    "for column in categorical_columns:\n",
    "    df_merged[column] = df_merged[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing continuous data \n",
    "# This is done by selecting columns with a numeric type - float64 or int64\n",
    "continuous_columns = df_merged.select_dtypes(['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(convert_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Day of Week Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['day_of_week'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_day_of_week(row['dayofservice']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Weekend/Weekday Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['weekday'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_weekday_weekend(row['dayofservice']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Bank Holiday Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make list of bank holidays for 2018 (based on https://www.officeholidays.com/countries/ireland/2018):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = ['2018-01-01 00:00:00', '2018-03-19 00:00:00', '2018-04-02 00:00:00', '2018-05-07 00:00:00', '2018-06-04 00:00:00',\\\n",
    "           '2018-08-06 00:00:00', '2018-10-29 00:00:00', '2018-12-25 00:00:00', '2018-12-26 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature is added using a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['bank_holiday'] = df_merged.apply (lambda row: convert_timestamp.timestamp_to_bank_holiday(row['dayofservice'], \\\n",
    "                                                                                                      holidays), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Data Quality Plan - Bus Data (Before Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| tenderlot       | All rows are null | Drop feature |\n",
    "| suppressed_trip | All rows are null | Drop feature |\n",
    "| justificationid_trip | All rows are null | Drop feature |\n",
    "| passengers | All rows are null | Drop feature | \n",
    "| passengersin | All rows are null | Drop feature |\n",
    "| passengersout | All rows are null | Drop feature |\n",
    "| distance_leavetimes | All rows are null | Drop feature |\n",
    "| note_leavetimes | All rows are null | Drop feature |\n",
    "| note_vehicle | All rows are null | Drop feature |\n",
    "| datasource | Constant feature | Drop feature |\n",
    "| lineid | Constant feature | This is constant because we just have data for one route loaded. At some point we may process more than one route together so will keep feature for now. May not be needed to train the model. |\n",
    "| basin | Constant feature | Drop feature |\n",
    "| lastupdate_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| note_trip | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| suppressed_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| justifcationid_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| lastupdate_leavetimes | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| vehicleid | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| distance_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| minutes_vehicle | Cannot be used to train model as we won't be able to provide this information | Drop feature |\n",
    "| actualtime_arr_trip | Missing values < 1% | Ignore for now as this feature is not brought across when data is transformed. |\n",
    "| actualtime_dep_trip | Missing values < 3% | Ignore for now as this feature is not brought across when data is transformed. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Data Quality Plan - Bus Data (After Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| stoppointid_first | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_first | Missing values ~ 1%| Drop affected rows |\n",
    "| stoppointid_next | Missing values ~ 1% | Drop affected rows |\n",
    "| actualtime_arr_stop_next | Missing values ~ 1%| Drop affected rows |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 3: Data Quality Plan - Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Data Quality Issue | Handling Strategy |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| itemp | Constant feature | Drop feature |\n",
    "| iwb | Constant feature | Drop feature |\n",
    "| rain | Missing data - 2 rows | Imputation - replace with 0 after looking at data for other timestamps on the same day |\n",
    "| irain | Seems to be a missing data indicator | Drop feature as only two rows have missing data, and imputation is performed for these rows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 4: Tests for *transform_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df.loc[5:100]\n",
    "df_test1 = df_test1.reset_index(drop=True)\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed1 = transform.transform_data(df_test1)\n",
    "df_transformed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:35], df[42:100]]\n",
    "df_test2 = pd.concat(pieces)\n",
    "df_test2 = df_test2.reset_index(drop=True)\n",
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed2 = transform.transform_data(df_test2)\n",
    "df_transformed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[10:100]]\n",
    "df_test3 = pd.concat(pieces)\n",
    "df_test3 = df_test3.reset_index(drop=True)\n",
    "df_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed3 = transform.transform_data(df_test3)\n",
    "df_transformed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pieces = [df[:5], df[8:10], df[14:50]]\n",
    "df_test4 = pd.concat(pieces)\n",
    "df_test4 = df_test4.reset_index(drop=True)\n",
    "df_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed4 = transform.transform_data(df_test4)\n",
    "df_transformed4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
